{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_hJeCIZNU73"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "colab_type": "code",
    "id": "JxwMgGPTDFBz",
    "outputId": "ae6389d0-5b31-4922-fc7d-52cbb32ed32f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-1b6ff1cc1ee8>:1: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "82eHmEYoERjo"
   },
   "outputs": [],
   "source": [
    "# parse label data\n",
    "\n",
    "def one_hot(oringinal_label):\n",
    "  datalength = len(oringinal_label)\n",
    "  label = np.zeros([datalength,10])\n",
    "  for i in range(len(oringinal_label)):\n",
    "    index = oringinal_label[i] \n",
    "    label[i,index]=1    \n",
    "  return label\n",
    "\n",
    "\n",
    "train_labels = one_hot(train_labels)\n",
    "eval_labels = one_hot(eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "PhlWs9xZUIc5",
    "outputId": "1aa48b77-be6c-4e20-b3f8-83c4d903c587"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHhQBf66EzZe"
   },
   "outputs": [],
   "source": [
    "# hidden layer\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    # add one more layer and return the output of this layer\n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size],stddev=0.01))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    Wx_plus_b = tf.nn.dropout(Wx_plus_b, rate=0.5)\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "11B95zzBHqco"
   },
   "outputs": [],
   "source": [
    "# convolutional and pooling layer\n",
    "def conv_layer(inputs, inputs_shape, nfilters):\n",
    "  new_inputs = tf.reshape(inputs, inputs_shape)\n",
    "  layer_output = tf.layers.conv2d(new_inputs, filters=nfilters, kernel_size=[5,5], padding='same')\n",
    "  return layer_output\n",
    "\n",
    "\n",
    "def maxpool(inputs):\n",
    "  layer_output = tf.nn.max_pool(inputs, [1,2,2,1], [1,2,2,1], 'VALID')\n",
    "  return layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6YA0TXggWfb4"
   },
   "outputs": [],
   "source": [
    "InputNeurons = 784\n",
    "OutputNeurons = 10\n",
    "drop_rate = 0.5  # dropout parameter, keep how many percentage\n",
    "learning_rate = 0.01\n",
    "printstep = 50\n",
    "traing_epochs = 701\n",
    "\n",
    "# define placeholder for inputs to network\n",
    "rate = tf.placeholder(tf.float32)\n",
    "xs = tf.placeholder(tf.float32, [None, InputNeurons])\n",
    "ys = tf.placeholder(tf.float32, [None, OutputNeurons])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the structure of the convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "8Xg7MDcRWm5G",
    "outputId": "91521d4b-1c8c-4bb7-a491-7605d47efc62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-cf057b69777a>:3: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# add hidden layer\n",
    "conv1 = conv_layer(xs, [-1,28,28,1], 9)\n",
    "mp1 = maxpool(conv1)\n",
    "conv2 = conv_layer(mp1, [-1,14,14,1], 9)\n",
    "mp2 = maxpool(conv2)\n",
    "mp2 = tf.reshape(mp2, [-1,7*7*9*9])\n",
    "l1 = add_layer(mp2, 7*7*9*9, 100, activation_function=tf.nn.relu) \n",
    "prediction = add_layer(l1, 100, OutputNeurons, activation_function=tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2546
    },
    "colab_type": "code",
    "id": "kmG7tpfFWw8F",
    "outputId": "0030b98f-d3e4-4587-99b6-f28fcd512554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 validation loss= [2.2960289 2.2877831 2.3051462 ... 2.2949636 2.3135176 2.3066769] Train Loss= [2.292575  2.3136723 2.2901454 2.290981  2.3072474 2.3097942 2.2883775\n",
      " 2.3136477 2.311834  2.3136353 2.288071  2.3090732 2.2972393 2.2953444\n",
      " 2.2858863 2.3107464 2.2930558 2.2840364 2.3043816 2.3152888 2.3134217\n",
      " 2.3116295 2.3090253 2.308257  2.3143618 2.3104026 2.3106887 2.3097122\n",
      " 2.294181  2.3128924 2.2882855 2.3129556 2.29972   2.3157697 2.2861457\n",
      " 2.2923927 2.2957098 2.3096204 2.286984  2.3127615 2.299711  2.2928395\n",
      " 2.2897594 2.3093054 2.2871878 2.3063679 2.3163524 2.2886393 2.3122463\n",
      " 2.313577  2.2898605 2.2927845 2.2795823 2.3141525 2.2916133 2.296153\n",
      " 2.3167343 2.281901  2.3150797 2.3078153 2.2942948 2.2843156 2.287541\n",
      " 2.3151588]\n",
      "epoch 50 validation loss= [2.3855221 2.3731287 1.4803826 ... 2.3620608 2.377378  2.2892919] Train Loss= [2.351445  2.3036413 2.4142869 2.3920321 2.4469805 2.3022225 2.3788671\n",
      " 2.327118  2.3292158 2.159106  2.271593  2.2755232 2.005219  2.3779917\n",
      " 2.4326158 2.3500655 1.6464902 2.349977  2.37775   2.3096778 2.3651373\n",
      " 2.3777366 2.2732675 2.2866008 2.3445997 2.3221319 2.1117368 2.1400716\n",
      " 2.276307  2.3422408 2.3712392 2.367951  1.9095459 2.4497993 2.3106337\n",
      " 2.3697534 2.1738076 2.174674  2.3619056 2.3970244 1.656373  1.4680172\n",
      " 2.3347914 2.1821394 2.294529  2.3434544 2.3079133 2.3358028 1.7264626\n",
      " 2.3158743 2.4227998 1.4623569 2.4182994 2.3415868 2.3508623 1.4804412\n",
      " 1.5251579 2.3044    2.1149917 1.8000581 1.4689544 2.422102  2.2829487\n",
      " 2.4275568]\n",
      "epoch 100 validation loss= [2.3334057 1.4793339 2.2801445 ... 2.446031  2.3991945 1.4612062] Train Loss= [1.9806328 2.264511  2.3383646 1.461154  1.6563408 1.4946979 2.0860841\n",
      " 2.3034582 1.652858  1.4618169 1.4618207 2.2250597 2.4576075 2.268421\n",
      " 1.5321982 1.4612378 2.2708585 2.2603667 1.5170673 1.4647839 1.4802834\n",
      " 2.4027803 2.2322216 1.5551574 1.4613659 2.0939448 2.3890073 1.749447\n",
      " 2.4501455 2.223733  2.2924027 2.0939445 2.2273355 2.4456995 1.4723828\n",
      " 1.462736  1.4622989 1.4631404 2.2524714 1.4668767 2.4461713 2.3839314\n",
      " 1.4615768 1.4627808 2.4260426 1.5310098 1.4612395 1.4616374 2.249581\n",
      " 2.437445  1.5225885 2.302503  1.4805734 2.4403276 1.4675885 2.2625055\n",
      " 1.5707816 2.3205152 1.4958427 1.4612461 2.2725322 1.4703469 2.4584727\n",
      " 2.3749397]\n",
      "epoch 150 validation loss= [2.2816145 2.4454384 2.2793326 ... 2.3987691 2.3633592 1.4611502] Train Loss= [2.2459872 1.9369557 1.4611739 1.4788481 1.7326517 2.4610782 1.4714006\n",
      " 1.4674038 2.4323778 2.2495997 2.386988  2.4495993 2.2808094 2.2166705\n",
      " 2.2788239 1.463251  2.2418442 1.627824  2.3327346 2.2802556 1.4634176\n",
      " 1.4611523 2.4387867 1.461151  1.9659052 2.4499257 1.4617417 2.2770505\n",
      " 2.1618116 2.3815947 2.4075007 1.4638418 1.6169711 2.240901  2.4596705\n",
      " 2.4201632 2.2408903 1.4611646 2.213161  2.3329043 1.4615061 1.4611502\n",
      " 1.4617132 1.5583012 2.21369   1.6949421 2.2406049 1.6355861 1.4682511\n",
      " 2.1682649 1.4611698 2.2933187 1.4611646 2.1980507 2.2852683 2.1963618\n",
      " 1.4613345 1.4629115 2.212639  1.8204931 2.461103  2.1721082 2.4529374\n",
      " 2.4188933]\n",
      "epoch 200 validation loss= [2.278815  2.4600916 2.2789361 ... 2.453658  2.3202212 1.4611515] Train Loss= [2.4109035 2.2392402 1.4612011 2.239372  2.213293  2.4552045 2.2788177\n",
      " 2.2791107 1.8373698 2.397964  1.6422498 1.4688082 2.2395203 1.4634249\n",
      " 2.281583  2.2448637 2.3301044 2.2428777 2.4540145 1.4611934 2.4494085\n",
      " 2.4522572 2.384275  1.461166  2.4485483 2.2959685 1.4611502 2.134335\n",
      " 1.4611794 1.4612776 1.4646322 1.8015585 2.4108305 1.4611504 1.4646324\n",
      " 2.3919508 2.2736533 1.5799911 1.5440302 2.243254  1.4615026 1.5043306\n",
      " 1.5392547 1.4612296 2.2048237 1.5374465 2.0821786 1.9771485 1.4848903\n",
      " 2.262151  1.4778113 1.46115   1.4611723 2.37267   2.1782935 1.4611557\n",
      " 1.5533    1.4611962 1.7913008 1.4736485 1.4611615 1.5603256 2.4359617\n",
      " 2.3885007]\n",
      "epoch 250 validation loss= [1.4611502 2.222732  2.2131371 ... 2.4609697 2.24106   2.2982886] Train Loss= [1.4676204 2.3815296 2.2409947 1.5267005 2.4611497 2.3421588 2.2392764\n",
      " 2.2392874 2.2412    2.287439  1.461154  1.4620867 2.2622814 1.5518011\n",
      " 2.345358  2.2392125 1.4918553 2.2079625 1.4612334 1.4629399 1.4611626\n",
      " 1.7461994 2.2367015 1.461246  1.4612463 1.461558  1.461258  2.4609895\n",
      " 1.4753075 1.738857  1.4709355 2.2080052 2.459338  2.227436  2.2451143\n",
      " 1.4626293 2.198833  2.2788353 1.4633003 2.2671373 2.2618713 2.2077246\n",
      " 1.4611585 1.4698942 1.4795202 1.4661258 2.2920134 1.4611539 1.4611577\n",
      " 1.4882443 1.5457194 2.2081895 2.3042228 2.3078966 1.4612427 1.5347126\n",
      " 2.3932092 1.4611816 2.2151074 2.1955814 1.8953111 2.4342785 1.4614437\n",
      " 2.2093499]\n",
      "epoch 300 validation loss= [1.4611502 2.1721864 2.0860314 ... 2.302163  1.5492935 2.4516933] Train Loss= [1.4759557 2.4582672 1.4620831 2.294002  2.426297  2.2392297 2.43879\n",
      " 1.46115   1.4619406 1.4612465 2.2788038 2.3373373 1.4611502 2.3478293\n",
      " 2.3033342 2.459929  1.4611554 1.4611503 2.239243  2.3813787 2.2262888\n",
      " 1.4611526 2.2686675 2.262047  1.4611504 2.278804  1.4611502 1.4632012\n",
      " 2.2407386 1.8481178 2.2628899 1.6976149 2.4515486 1.4611527 2.2110403\n",
      " 2.4146585 1.4615568 2.2208507 1.4718486 2.2271085 1.4619707 1.46115\n",
      " 1.4620328 1.4611502 1.4613271 1.5238702 1.4612461 1.46115   1.4633014\n",
      " 2.2788503 1.4611548 2.162878  2.4611254 1.4614847 1.4611533 2.266439\n",
      " 2.4132812 2.3875177 2.2402618 1.47154   2.2622304 1.4611517 2.2618327\n",
      " 1.4656583]\n",
      "epoch 350 validation loss= [2.2920136 2.2622743 2.2113192 ... 2.245403  1.5778303 2.2392228] Train Loss= [1.4630313 2.278829  2.1738737 1.4611517 2.2287698 2.0899472 2.207838\n",
      " 1.4766558 1.4611802 2.2721088 1.4616945 2.239686  2.2088711 1.46117\n",
      " 2.4611466 1.4611627 2.2618513 2.3687868 2.2075882 1.4611502 2.285085\n",
      " 2.232773  2.3662925 2.222245  1.5419561 1.4611502 2.374742  2.2502735\n",
      " 2.261835  2.2264795 1.4818258 1.4651818 2.2779288 2.226183  1.4611883\n",
      " 1.4611502 2.1200428 2.4158497 2.2546432 2.4611502 1.4620521 1.461151\n",
      " 1.4611502 2.460151  1.4611502 2.2116196 1.6539674 2.2817082 2.2125154\n",
      " 2.2396307 1.461152  1.4612077 2.2619483 2.1999328 2.3993616 2.2788086\n",
      " 1.463361  2.2630768 2.0815647 2.2921863 1.4611582 2.262825  1.4628248\n",
      " 2.2618325]\n",
      "epoch 400 validation loss= [2.093171  1.4629451 1.4612858 ... 1.4611514 1.4621061 2.4360898] Train Loss= [2.2544613 1.4611577 2.1787066 2.2618794 2.2431047 1.5073513 1.4820864\n",
      " 2.2098353 2.20799   2.3767366 2.1751776 2.2089424 2.2613661 2.185942\n",
      " 2.2188663 2.2804728 2.2394297 2.24463   2.4602768 1.543712  2.2077825\n",
      " 2.2076788 1.9645714 2.252366  1.4619784 2.2398014 1.4611628 1.4612982\n",
      " 2.2099814 2.263331  1.4611771 2.4611502 1.5033792 1.4611505 2.459382\n",
      " 2.4585648 2.2075787 1.4611506 2.4495127 1.4611542 1.4611723 2.2431083\n",
      " 2.4325612 1.5431774 2.266576  1.4732989 2.2864313 1.4656955 2.4611495\n",
      " 1.4611596 2.3127093 2.3888497 2.2634845 2.240212  2.2393506 2.4405308\n",
      " 1.4793675 1.4633954 2.4048238 2.1608074 1.4644556 1.4611615 2.402109\n",
      " 1.4611502]\n",
      "epoch 450 validation loss= [2.183295  1.4612627 2.2397232 ... 1.4626229 2.3668303 2.2392116] Train Loss= [1.4611534 1.4628804 2.447811  2.239212  1.4630234 2.4574351 1.5174155\n",
      " 2.1601956 2.231662  2.2990355 1.4612545 1.4611514 1.4639139 1.4611505\n",
      " 1.4794136 1.5504547 1.4611515 1.5175287 1.4611502 2.2261062 1.4611624\n",
      " 1.4612397 2.2392116 1.4611502 2.3115396 1.4611502 2.2456844 2.241029\n",
      " 2.4379797 2.272279  2.2524312 2.16052   2.1626272 1.4703449 1.4613713\n",
      " 1.64656   1.4611565 1.4611503 1.6437042 1.474074  2.2153237 1.461154\n",
      " 1.461152  2.2392266 2.2788038 1.4612768 2.2788048 1.4622464 2.459728\n",
      " 2.2583125 2.2677028 1.5652641 1.4611713 1.4611506 1.4611748 1.4611511\n",
      " 2.207588  1.46115   2.4601612 2.2516716 2.247202  2.2347538 2.169324\n",
      " 2.4608448]\n",
      "epoch 500 validation loss= [2.3805206 1.4612075 1.4611679 ... 2.163146  2.2788043 1.4611502] Train Loss= [2.4500234 1.4611504 2.208478  2.2430468 1.4843382 2.2167296 2.1603456\n",
      " 1.5857373 2.2556434 2.1636045 2.2618828 2.254656  2.2392201 2.2332633\n",
      " 2.2395718 1.5119286 1.4611534 2.278804  1.4870819 2.2082148 2.36645\n",
      " 1.4701489 1.7505232 1.4611751 2.271061  1.4701998 2.4552107 2.2393188\n",
      " 1.4611722 1.5353711 2.3372571 2.4608402 1.4644845 1.46115   1.9517304\n",
      " 2.2788055 2.388916  2.2618291 1.4631451 2.4589477 2.2392597 2.451733\n",
      " 1.4611502 1.7070415 2.2407384 2.2093956 2.2799947 1.4611502 2.2141604\n",
      " 1.4612305 2.4221973 2.2618299 1.4611572 2.207581  2.2614424 2.239353\n",
      " 2.2084496 1.46115   1.5350277 2.2200313 1.4660288 2.2788112 1.8276241\n",
      " 2.2619123]\n",
      "epoch 550 validation loss= [2.2299204 2.2418666 2.1621723 ... 2.2396092 1.4628129 2.2455847] Train Loss= [1.4611506 2.2136083 1.46115   2.4300323 2.240141  2.2392416 2.1607718\n",
      " 1.464117  2.4573636 2.4446363 2.281349  2.2618954 2.2618313 1.46115\n",
      " 2.1666198 2.1602335 2.0555031 2.2996287 2.261833  2.208375  1.4611504\n",
      " 2.1647804 2.4221866 2.31549   1.461463  2.2137878 2.4475777 2.1603167\n",
      " 2.2587245 1.4614618 2.2392273 1.461531  2.4009805 1.4611502 2.2503755\n",
      " 2.2076035 1.9443521 2.2618668 1.4611502 2.4517875 2.2789571 2.2541962\n",
      " 1.4657937 1.4611506 1.4611502 2.082592  2.2794147 1.461153  2.215882\n",
      " 2.2392952 1.4613734 2.2098126 2.4443152 1.4611542 2.2467988 2.2393053\n",
      " 1.4612023 1.4611506 1.4629312 1.4611515 2.2631512 2.2140918 2.310348\n",
      " 2.207585 ]\n",
      "epoch 600 validation loss= [2.1605577 2.097938  2.0827224 ... 2.2618482 2.221823  2.207753 ] Train Loss= [2.160199  1.4611628 2.1143465 1.46115   2.1726372 1.4611542 2.4554443\n",
      " 1.4620341 1.4611503 2.2472951 1.461152  1.4611534 1.4613209 2.208515\n",
      " 2.2079108 2.207762  2.3843594 1.4611512 1.4612198 1.4611502 2.2640498\n",
      " 1.4611592 1.461256  2.2119007 1.4611502 2.2659411 2.2920415 1.4612938\n",
      " 2.382299  1.46115   2.4056728 1.4612074 1.5279359 2.2434826 1.4613229\n",
      " 1.4614125 2.4585636 1.4677484 1.4611502 2.4026186 1.4851567 1.4611505\n",
      " 2.431281  2.239255  1.4669753 2.4526734 2.3910565 2.2395997 1.4611509\n",
      " 1.4611506 2.433138  2.2111406 2.1691923 2.1750922 1.46115   2.2618346\n",
      " 2.2126548 2.2266605 2.2127857 2.27905   2.162275  2.2618992 2.2399607\n",
      " 1.4615732]\n",
      "epoch 650 validation loss= [1.4611502 1.4623542 2.2077045 ... 2.2392116 2.239294  2.1601932] Train Loss= [1.5244033 1.4707166 1.4611502 2.3904243 1.4611504 1.4635402 2.2392662\n",
      " 1.6458703 1.4611504 1.461645  1.4613006 2.2075806 2.4595625 2.4525087\n",
      " 1.4611502 2.3977144 2.2920198 1.4611502 1.4714011 2.299165  2.2076285\n",
      " 2.110774  2.4562378 2.240879  1.4611517 1.9603057 2.1966686 2.2788043\n",
      " 1.461994  2.4377334 1.4611539 2.207577  2.3973074 2.2392116 2.2894998\n",
      " 1.4611502 1.4663535 1.5504857 2.2473292 2.2618306 1.4611502 1.4611524\n",
      " 2.240172  2.2648282 1.4611801 1.4611624 1.4611503 2.1626458 2.2588024\n",
      " 1.4611504 2.2621987 1.4611502 2.239793  1.46115   2.1616488 1.4611502\n",
      " 2.2401595 1.4611554 2.2392116 1.46115   2.2985566 2.2505078 2.369039\n",
      " 1.4611502]\n",
      "epoch 700 validation loss= [1.4611502 1.4629945 2.2076197 ... 2.2407389 2.3341947 2.2618291] Train Loss= [2.4105673 1.4612445 1.4757166 1.4611517 2.2081141 2.3780324 2.2991002\n",
      " 2.2920134 2.457749  1.4611866 1.4635748 2.207633  1.4611528 1.4638077\n",
      " 2.391517  1.4611502 2.2093976 2.2618291 2.3860896 1.4611819 2.4358501\n",
      " 1.4611588 1.46115   1.46115   1.46115   1.46115   1.5540727 1.4611723\n",
      " 2.2499359 2.2392116 2.3054245 2.1752217 2.207975  2.4444041 2.4538355\n",
      " 1.4611509 1.4635257 2.1724164 2.2901576 1.4611522 1.4726056 1.4611628\n",
      " 1.4611502 2.4371731 1.4611515 1.4612093 2.2076313 2.2081335 2.1779416\n",
      " 2.2788038 2.2480502 2.3179054 1.4624267 1.5691954 1.4611913 1.4612598\n",
      " 2.2619157 1.461154  2.2413538 1.4874108 1.4732502 2.4149408 1.4739925\n",
      " 2.237158 ]\n"
     ]
    }
   ],
   "source": [
    "# the error between prediction and real data\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), axis=1))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=ys,logits=prediction,axis=-1)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# start of the neural network\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(traing_epochs):\n",
    "    # training\n",
    "    idx = np.random.choice(np.arange(len(train_data)), 64, replace=False)\n",
    "    x_train_batch = train_data[idx]\n",
    "    y_train_batch = train_labels[idx]\n",
    "    sess.run(train_step, feed_dict={xs: x_train_batch, ys: y_train_batch, rate: drop_rate})\n",
    "    if i % printstep == 0:\n",
    "        # to see the step improvement\n",
    "        train_loss = sess.run(loss, feed_dict={xs: x_train_batch, ys: y_train_batch, rate: 1})\n",
    "        validation_loss = sess.run(loss, feed_dict={xs: eval_data, ys: eval_labels, rate:1})\n",
    "        print(\"epoch\",i,\"validation loss=\", validation_loss,\"Train Loss=\",train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aYjmuCwAc3oj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYuITXwRclFX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_ken.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
