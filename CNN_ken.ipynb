{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_ken.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_hJeCIZNU73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxwMgGPTDFBz",
        "colab_type": "code",
        "outputId": "19b28161-cb10-49d7-ee63-f9a269bf142c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "source": [
        "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
        "train_data = mnist.train.images  # Returns np.array\n",
        "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
        "eval_data = mnist.test.images  # Returns np.array\n",
        "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-2-1b6ff1cc1ee8>:1: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82eHmEYoERjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parse label data\n",
        "\n",
        "def one_hot(oringinal_label):\n",
        "  datalength = len(oringinal_label)\n",
        "  label = np.zeros([datalength,10])\n",
        "  for i in range(len(oringinal_label)):\n",
        "    index = oringinal_label[i] \n",
        "    label[i,index]=1    \n",
        "  return label\n",
        "\n",
        "\n",
        "train_labels = one_hot(train_labels)\n",
        "eval_labels = one_hot(eval_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhlWs9xZUIc5",
        "colab_type": "code",
        "outputId": "85ce4c82-22f8-4506-ce21-aefc53ee5b6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "eval_labels[100,:]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV3wcXy7wAX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define accuracy\n",
        "def accuracy(labels, pred):\n",
        "  # both labels and pred are in the shape of one-hot encoding\n",
        "  accu=0\n",
        "  for i in range(len(labels)):\n",
        "    lab_val = np.argmax(labels[i,:])\n",
        "    pred_val = np.argmax(pred[i,:])\n",
        "    if lab_val==pred_val:\n",
        "      accu+=1\n",
        "  return accu/len(labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHhQBf66EzZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hidden layer\n",
        "def add_layer(inputs, in_size, out_size, rate, activation_function=None):\n",
        "    # add one more layer and return the output of this layer\n",
        "    Weights = tf.Variable(tf.random_normal([in_size, out_size],stddev=0.01))\n",
        "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
        "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
        "    Wx_plus_b = tf.nn.dropout(Wx_plus_b, rate)\n",
        "    if activation_function is None:\n",
        "        outputs = Wx_plus_b\n",
        "    else:\n",
        "        outputs = activation_function(Wx_plus_b)\n",
        "    outputs = tf.nn.dropout(outputs, rate)\n",
        "    # rate=1-keep_prop\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11B95zzBHqco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convolutional and pooling layer\n",
        "def conv_layer(inputs, inputs_shape, nfilters):\n",
        "  new_inputs = tf.reshape(inputs, inputs_shape)\n",
        "  layer_output = tf.layers.conv2d(new_inputs, filters=nfilters, kernel_size=[5,5], padding='same')\n",
        "  return layer_output\n",
        "\n",
        "\n",
        "def maxpool(inputs):\n",
        "  layer_output = tf.nn.max_pool(inputs, [1,2,2,1], [1,2,2,1], 'VALID')\n",
        "  return layer_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YA0TXggWfb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "InputNeurons = 784\n",
        "OutputNeurons = 10\n",
        "drop_rate = 0.5  # dropout parameter, keep how many percentage\n",
        "learning_rate = 0.01\n",
        "printstep = 50\n",
        "traing_epochs = 5001\n",
        "\n",
        "# define placeholder for inputs to network\n",
        "rate = tf.placeholder(tf.float32)\n",
        "xs = tf.placeholder(tf.float32, [None, InputNeurons])\n",
        "ys = tf.placeholder(tf.float32, [None, OutputNeurons])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xg7MDcRWm5G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ae1a1063-df5d-4c2e-e1e8-bb45620be625"
      },
      "source": [
        "# neural network structure\n",
        "conv1 = conv_layer(xs, [-1,28,28,1], 9)\n",
        "mp1 = maxpool(conv1)\n",
        "conv2 = conv_layer(mp1, [-1,14,14,1], 9)\n",
        "mp2 = maxpool(conv2)\n",
        "mp2 = tf.reshape(mp2, [-1,7*7*9*9])\n",
        "l1 = add_layer(mp2, 7*7*9*9, 100, rate, activation_function=tf.nn.relu)\n",
        "logits = add_layer(l1, 100, OutputNeurons, rate, activation_function=None)\n",
        "prediction = tf.nn.softmax(logits)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-cf057b69777a>:3: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-5-e3a876aa36f3>:6: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmG7tpfFWw8F",
        "colab_type": "code",
        "outputId": "8a2b009c-3ca6-4b7a-b462-2317b35d41f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1860
        }
      },
      "source": [
        "# the error between prediction and real data\n",
        "#loss = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), axis=1))\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ys, logits=logits))\n",
        "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "# start of the neural network\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "for i in range(traing_epochs):\n",
        "    # training\n",
        "    idx = np.random.choice(np.arange(len(train_data)), 64, replace=False)\n",
        "    x_train_batch = train_data[idx]\n",
        "    y_train_batch = train_labels[idx]\n",
        "    sess.run(train_step, feed_dict={xs: x_train_batch, ys: y_train_batch, rate: drop_rate})\n",
        "    if i % printstep == 0:\n",
        "        # to see the step improvement\n",
        "        train_loss = sess.run(loss, feed_dict={xs: x_train_batch, ys: y_train_batch, rate: 1})\n",
        "        validation_loss = sess.run(loss, feed_dict={xs: eval_data, ys: eval_labels, rate:1})\n",
        "        print(\"epoch\",i,\"validation loss=\", validation_loss,\"Train Loss=\",train_loss)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-d0a82b667e2f>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "epoch 0 validation loss= 2.302743 Train Loss= 2.300772\n",
            "epoch 50 validation loss= 2.298849 Train Loss= 2.297701\n",
            "epoch 100 validation loss= 2.2948644 Train Loss= 2.2936587\n",
            "epoch 150 validation loss= 2.2862446 Train Loss= 2.2803488\n",
            "epoch 200 validation loss= 2.2715843 Train Loss= 2.2691715\n",
            "epoch 250 validation loss= 2.2375126 Train Loss= 2.2359164\n",
            "epoch 300 validation loss= 2.1545868 Train Loss= 2.1499424\n",
            "epoch 350 validation loss= 2.0093887 Train Loss= 1.9957831\n",
            "epoch 400 validation loss= 1.8725879 Train Loss= 1.8647704\n",
            "epoch 450 validation loss= 1.7029461 Train Loss= 1.7338867\n",
            "epoch 500 validation loss= 1.6413615 Train Loss= 1.7331532\n",
            "epoch 550 validation loss= 1.5715687 Train Loss= 1.603802\n",
            "epoch 600 validation loss= 1.4853343 Train Loss= 1.5517092\n",
            "epoch 650 validation loss= 1.4588418 Train Loss= 1.5231688\n",
            "epoch 700 validation loss= 1.3840125 Train Loss= 1.3247573\n",
            "epoch 750 validation loss= 1.3501513 Train Loss= 1.3224759\n",
            "epoch 800 validation loss= 1.273965 Train Loss= 1.3544664\n",
            "epoch 850 validation loss= 1.2511406 Train Loss= 1.3829393\n",
            "epoch 900 validation loss= 1.230626 Train Loss= 1.346425\n",
            "epoch 950 validation loss= 1.2082928 Train Loss= 1.2789723\n",
            "epoch 1000 validation loss= 1.1353469 Train Loss= 1.1691318\n",
            "epoch 1050 validation loss= 1.1208439 Train Loss= 1.067543\n",
            "epoch 1100 validation loss= 1.1817017 Train Loss= 1.2444011\n",
            "epoch 1150 validation loss= 1.0846949 Train Loss= 1.0532383\n",
            "epoch 1200 validation loss= 1.0975593 Train Loss= 1.1313416\n",
            "epoch 1250 validation loss= 1.0192482 Train Loss= 1.0855821\n",
            "epoch 1300 validation loss= 1.0762076 Train Loss= 1.1383126\n",
            "epoch 1350 validation loss= 1.0004826 Train Loss= 1.0355811\n",
            "epoch 1400 validation loss= 0.98296815 Train Loss= 1.0786512\n",
            "epoch 1450 validation loss= 0.95032185 Train Loss= 0.9307227\n",
            "epoch 1500 validation loss= 0.9889349 Train Loss= 1.0637419\n",
            "epoch 1550 validation loss= 0.9429531 Train Loss= 0.95115644\n",
            "epoch 1600 validation loss= 0.9707897 Train Loss= 1.0112308\n",
            "epoch 1650 validation loss= 0.9289572 Train Loss= 0.9910716\n",
            "epoch 1700 validation loss= 0.9287674 Train Loss= 0.9884462\n",
            "epoch 1750 validation loss= 0.93123585 Train Loss= 1.0162619\n",
            "epoch 1800 validation loss= 0.86655486 Train Loss= 0.8481199\n",
            "epoch 1850 validation loss= 0.8860483 Train Loss= 0.89191985\n",
            "epoch 1900 validation loss= 0.9344534 Train Loss= 0.9798801\n",
            "epoch 1950 validation loss= 0.8934357 Train Loss= 0.8860594\n",
            "epoch 2000 validation loss= 0.8339186 Train Loss= 0.86777323\n",
            "epoch 2050 validation loss= 0.83563066 Train Loss= 0.92026025\n",
            "epoch 2100 validation loss= 0.8007817 Train Loss= 0.78707415\n",
            "epoch 2150 validation loss= 0.81550443 Train Loss= 0.7294687\n",
            "epoch 2200 validation loss= 0.8183467 Train Loss= 0.7617761\n",
            "epoch 2250 validation loss= 0.80116934 Train Loss= 0.81177473\n",
            "epoch 2300 validation loss= 0.74970263 Train Loss= 0.8225248\n",
            "epoch 2350 validation loss= 0.7724704 Train Loss= 0.84550977\n",
            "epoch 2400 validation loss= 0.77389973 Train Loss= 0.802896\n",
            "epoch 2450 validation loss= 0.7594981 Train Loss= 0.72151375\n",
            "epoch 2500 validation loss= 0.7552139 Train Loss= 0.7477218\n",
            "epoch 2550 validation loss= 0.7765579 Train Loss= 0.86042535\n",
            "epoch 2600 validation loss= 0.7608182 Train Loss= 0.7371748\n",
            "epoch 2650 validation loss= 0.7057145 Train Loss= 0.7621991\n",
            "epoch 2700 validation loss= 0.69757825 Train Loss= 0.6951268\n",
            "epoch 2750 validation loss= 0.69531345 Train Loss= 0.66235787\n",
            "epoch 2800 validation loss= 0.7103675 Train Loss= 0.7563612\n",
            "epoch 2850 validation loss= 0.66791 Train Loss= 0.67094827\n",
            "epoch 2900 validation loss= 0.67060745 Train Loss= 0.7345026\n",
            "epoch 2950 validation loss= 0.68748987 Train Loss= 0.79808825\n",
            "epoch 3000 validation loss= 0.6546637 Train Loss= 0.5829655\n",
            "epoch 3050 validation loss= 0.66915387 Train Loss= 0.65911853\n",
            "epoch 3100 validation loss= 0.6628257 Train Loss= 0.77055794\n",
            "epoch 3150 validation loss= 0.65255374 Train Loss= 0.7872398\n",
            "epoch 3200 validation loss= 0.6899151 Train Loss= 0.66420543\n",
            "epoch 3250 validation loss= 0.6527617 Train Loss= 0.6071869\n",
            "epoch 3300 validation loss= 0.63043797 Train Loss= 0.6624076\n",
            "epoch 3350 validation loss= 0.626475 Train Loss= 0.5717965\n",
            "epoch 3400 validation loss= 0.6148807 Train Loss= 0.66776806\n",
            "epoch 3450 validation loss= 0.6144222 Train Loss= 0.6787127\n",
            "epoch 3500 validation loss= 0.63676494 Train Loss= 0.6917473\n",
            "epoch 3550 validation loss= 0.6203953 Train Loss= 0.59139186\n",
            "epoch 3600 validation loss= 0.61276513 Train Loss= 0.64531565\n",
            "epoch 3650 validation loss= 0.6272098 Train Loss= 0.62928164\n",
            "epoch 3700 validation loss= 0.6138085 Train Loss= 0.5988891\n",
            "epoch 3750 validation loss= 0.6090475 Train Loss= 0.6311497\n",
            "epoch 3800 validation loss= 0.61563885 Train Loss= 0.68202287\n",
            "epoch 3850 validation loss= 0.6064175 Train Loss= 0.6839042\n",
            "epoch 3900 validation loss= 0.60969424 Train Loss= 0.6273725\n",
            "epoch 3950 validation loss= 0.5901904 Train Loss= 0.6450683\n",
            "epoch 4000 validation loss= 0.573578 Train Loss= 0.5430653\n",
            "epoch 4050 validation loss= 0.5849695 Train Loss= 0.62268364\n",
            "epoch 4100 validation loss= 0.56739897 Train Loss= 0.58909637\n",
            "epoch 4150 validation loss= 0.5845096 Train Loss= 0.6296165\n",
            "epoch 4200 validation loss= 0.58331114 Train Loss= 0.6492088\n",
            "epoch 4250 validation loss= 0.5708008 Train Loss= 0.5558837\n",
            "epoch 4300 validation loss= 0.5435031 Train Loss= 0.5845289\n",
            "epoch 4350 validation loss= 0.54636496 Train Loss= 0.662857\n",
            "epoch 4400 validation loss= 0.5414397 Train Loss= 0.4421007\n",
            "epoch 4450 validation loss= 0.5372913 Train Loss= 0.48691165\n",
            "epoch 4500 validation loss= 0.53061104 Train Loss= 0.48588997\n",
            "epoch 4550 validation loss= 0.52358204 Train Loss= 0.63380575\n",
            "epoch 4600 validation loss= 0.53096604 Train Loss= 0.4825958\n",
            "epoch 4650 validation loss= 0.52698255 Train Loss= 0.5877645\n",
            "epoch 4700 validation loss= 0.5202631 Train Loss= 0.5755078\n",
            "epoch 4750 validation loss= 0.52419627 Train Loss= 0.59954953\n",
            "epoch 4800 validation loss= 0.5131938 Train Loss= 0.5384921\n",
            "epoch 4850 validation loss= 0.53619534 Train Loss= 0.60823977\n",
            "epoch 4900 validation loss= 0.5258336 Train Loss= 0.6350713\n",
            "epoch 4950 validation loss= 0.5190421 Train Loss= 0.5785598\n",
            "epoch 5000 validation loss= 0.5014042 Train Loss= 0.46370986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYjmuCwAc3oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_prediction = sess.run(prediction, feed_dict={xs: eval_data, ys: eval_labels, rate: 1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAajuoB9qpD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e13de956-104d-4b03-a562-84ffe39202e5"
      },
      "source": [
        "print('vaildation data accuracy: ', accuracy(eval_labels, eval_prediction))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vaildation data accuracy:  0.9554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPV4pg8z5GJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}