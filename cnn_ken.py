# -*- coding: utf-8 -*-
"""CNN_ken.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pHiMCzPuVSJd0ntdQZrws_JdgeYGxUUu
"""

import tensorflow as tf
import numpy as np

mnist = tf.contrib.learn.datasets.load_dataset("mnist")
train_data = mnist.train.images  # Returns np.array
train_labels = np.asarray(mnist.train.labels, dtype=np.int32)
eval_data = mnist.test.images  # Returns np.array
eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)

# parse label data

def one_hot(oringinal_label):
  datalength = len(oringinal_label)
  label = np.zeros([datalength,10])
  for i in range(len(oringinal_label)):
    index = oringinal_label[i] 
    label[i,index]=1    
  return label


train_labels = one_hot(train_labels)
eval_labels = one_hot(eval_labels)

eval_labels[100,:]

# define accuracy
def accuracy(labels, pred):
  # both labels and pred are in the shape of one-hot encoding
  accu=0
  for i in range(len(labels)):
    lab_val = np.argmax(labels[i,:])
    pred_val = np.argmax(pred[i,:])
    if lab_val==pred_val:
      accu+=1
  return accu/len(labels)

# hidden layer
def add_layer(inputs, in_size, out_size, rate, activation_function=None):
    # add one more layer and return the output of this layer
    Weights = tf.Variable(tf.random_normal([in_size, out_size],stddev=0.01))
    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)
    Wx_plus_b = tf.matmul(inputs, Weights) + biases
    Wx_plus_b = tf.nn.dropout(Wx_plus_b, rate)
    if activation_function is None:
        outputs = Wx_plus_b
    else:
        outputs = activation_function(Wx_plus_b)
    outputs = tf.nn.dropout(outputs, rate)
    # rate=1-keep_prop
    return outputs

# convolutional and pooling layer
def conv_layer(inputs, inputs_shape, nfilters):
  new_inputs = tf.reshape(inputs, inputs_shape)
  layer_output = tf.layers.conv2d(new_inputs, filters=nfilters, kernel_size=[5,5], padding='same')
  return layer_output


def maxpool(inputs):
  layer_output = tf.nn.max_pool(inputs, [1,2,2,1], [1,2,2,1], 'VALID')
  return layer_output

InputNeurons = 784
OutputNeurons = 10
drop_rate = 0.5  # dropout parameter, keep how many percentage
learning_rate = 0.01
printstep = 50
traing_epochs = 5001

# define placeholder for inputs to network
rate = tf.placeholder(tf.float32)
xs = tf.placeholder(tf.float32, [None, InputNeurons])
ys = tf.placeholder(tf.float32, [None, OutputNeurons])

# neural network structure
conv1 = conv_layer(xs, [-1,28,28,1], 9)
mp1 = maxpool(conv1)
conv2 = conv_layer(mp1, [-1,14,14,1], 9)
mp2 = maxpool(conv2)
mp2 = tf.reshape(mp2, [-1,7*7*9*9])
l1 = add_layer(mp2, 7*7*9*9, 100, rate, activation_function=tf.nn.relu)
logits = add_layer(l1, 100, OutputNeurons, rate, activation_function=None)
prediction = tf.nn.softmax(logits)

# the error between prediction and real data
#loss = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), axis=1))
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ys, logits=logits))
train_step = tf.train.AdagradOptimizer(learning_rate).minimize(loss)

# start of the neural network
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

for i in range(traing_epochs):
    # training
    idx = np.random.choice(np.arange(len(train_data)), 64, replace=False)
    x_train_batch = train_data[idx]
    y_train_batch = train_labels[idx]
    sess.run(train_step, feed_dict={xs: x_train_batch, ys: y_train_batch, rate: drop_rate})
    if i % printstep == 0:
        # to see the step improvement
        train_loss = sess.run(loss, feed_dict={xs: x_train_batch, ys: y_train_batch, rate: 1})
        validation_loss = sess.run(loss, feed_dict={xs: eval_data, ys: eval_labels, rate:1})
        print("epoch",i,"validation loss=", validation_loss,"Train Loss=",train_loss)

eval_prediction = sess.run(prediction, feed_dict={xs: eval_data, ys: eval_labels, rate: 1})

print('vaildation data accuracy: ', accuracy(eval_labels, eval_prediction))

